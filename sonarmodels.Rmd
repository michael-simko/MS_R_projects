---
title: "sonarmodels"
author: "Mike Simko"
date: "8/14/2018"
output: rmarkdown::html_vignette
---

```{r setup, include=FALSE}
library (caret)
library (mlbench)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(dev="png")
```
##Introduction

An exercise to evaluate various classification models in the CARET package using the sonar dataset.

The sonar dataset (used by Gorman and Sejnowski) has 208 rows and 61 columns of numeric values between 0 an 1 with a final nominal column called "Class" which is recorded as either "R" for rock or "M" for mine/metal. â€¢	http://www.ics.uci.edu/~mlearn/MLRepository.html 

#Import dataset

```{r import Sonar dataset}
#import the dataset "Sonar" located in mlbench
data(Sonar)
```

#Split data into training and testing sets

```{r set up data for running models}
#partition the data into a 70%/30% training/testing split
set.seed(123)
inTrain <- createDataPartition(y=Sonar$Class,
                               p=.70,
                               list=FALSE)
training <- Sonar[inTrain,]
testing <- Sonar[-inTrain,]
#show the number of rows in both training and testing sets
message("There are ", nrow (training), " rows in the training data set")
message("There are ", nrow (testing), " rows in the testing data set")
```

#Compare four common classification models to one another

```{r run partial least squares model}
#define train control parameters
ctrl <- trainControl(method="repeatedcv",
                     repeats=3,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary)
#create model using training set data and with given parameters
plsFit <- train(Class ~.,
                data=training,
                method="pls",
                tuneLength=15,
                trControl=ctrl,
                metric="ROC",
                preProc=c("center","scale"))
#show results of pls modeling
plsFit
```

```{r evaluate pls model fit}
plsClasses <- predict(plsFit,newdata = testing)
str(plsClasses)
plot(plsFit)
plsProbs <- predict(plsFit,newdata = testing, type="prob")
head(plsProbs)
confusionMatrix(data=plsClasses, testing$Class)
```

```{r run regularized descriminant analysis model}
#define parameters to be used for rda model
rdaGrid <- data.frame(gamma=(0:4)/4,lambda=3/4)
rdaFit <- train(Class ~.,
                data=training,
                method="rda",
                tuneGrid=rdaGrid,
                trControl=ctrl,
                metric="ROC")
rdaFit
```

```{r evaluate rda model fit}
rdaClasses <- predict(rdaFit,newdata = testing)
str(rdaClasses)
plot(rdaFit)
rdaProbs <- predict(rdaFit,newdata = testing, type="prob")
head(rdaProbs)
confusionMatrix(data=rdaClasses, testing$Class)
```

```{r run k-nearest neighbors model}
#define parameters to be used for rda model
knnFit <- train(Class ~.,
                data=training,
                method="knn",
                trControl=ctrl,
                metric="ROC")
knnFit
```

```{r evaluate knn model fit}
knnClasses <- predict(knnFit,newdata = testing)
str(knnClasses)
plot(knnFit)
knnProbs <- predict(knnFit,newdata = testing, type="prob")
head(knnProbs)
confusionMatrix(data=knnClasses, testing$Class)
```

```{r run support vector machine model}
svmFit <- train(Class ~ ., data = training, 
                method = "svmRadial", 
                trControl = ctrl, 
                preProc = c("center", "scale"),
                tuneLength = 8,
                metric = "ROC")
svmFit 
```

```{r evaluate svm model fit}
svmClasses <- predict(svmFit,newdata = testing)
str(svmClasses)
plot(svmFit)
svmProbs <- predict(svmFit,newdata = testing, type="prob")
head(svmProbs)
confusionMatrix(data=svmClasses, testing$Class)

```

```{r compare four models}
resampall <- resamples(list(PLS = plsFit,
                          RDA = rdaFit,
                          KNN = knnFit,
                          SVM = svmFit))

resampall
summary(resampall)
#visualize comparison of model results
bwplot(resampall)
dotplot(resampall)
```

```{r between model comparison}
diffVals <- diff(resampall)
diffVals
summary(diffVals)
#boxplot
bwplot(diffVals, layout = c(3, 1))
#dotplot
dotplot(diffVals)

```

The results of these four models show that the support vector machine results in the best accuracy and kappa values after running the test set. k-nearest neighbor had the poorest performance.

#Create a set of gradient boosted models with various parameters

```{r run gradient boosted machine model}
set.seed(428)
fitControl <- trainControl(method = "repeatedcv", 
                           number = 10,
                           repeats = 10,
                           classProbs = TRUE)
gbmFit1 <- train(Class ~ ., data = training, 
                 method = "gbm", 
                 trControl = fitControl,
                 verbose = FALSE,
                 metric="Kappa")
gbmFit1
```

```{r evaluate gbm1 model fit}
gbm1Classes <- predict(gbmFit1,newdata = testing)
str(gbm1Classes)
plot(gbmFit1)
gbm1Probs <- predict(gbmFit1,newdata = testing, type="prob")
head(gbm1Probs)
confusionMatrix(data=gbm1Classes, testing$Class)
```

```{r run second gradient boosted machine model}
gbmGrid <-  expand.grid(interaction.depth = c(1, 5, 9), 
                        n.trees = (1:20)*25, 
                        shrinkage = 0.1,
                        n.minobsinnode = 20)
gbmFit2 <- train(Class ~ ., data = training, 
                 method = "gbm", 
                 trControl = fitControl, 
                 verbose = FALSE,
                 tuneGrid = gbmGrid,
                 metric = "Kappa")
gbmFit2
```

```{r evaluate gbm2 model fit}
gbm2Classes <- predict(gbmFit2,newdata = testing)
str(gbm2Classes)
plot(gbmFit2)
gbm2Probs <- predict(gbmFit2,newdata = testing, type="prob")
head(gbm2Probs)
confusionMatrix(data=gbm2Classes, testing$Class)
```

```{r run third gradient boosted machine model}
gbmGrid <-  expand.grid(interaction.depth = c(5, 10, 15), 
                        n.trees = (1:30)*50, 
                        shrinkage = 0.1,
                        n.minobsinnode = 20)
gbmFit3 <- train(Class ~ ., data = training, 
                 method = "gbm", 
                 trControl = fitControl, 
                 verbose = FALSE, 
                 tuneGrid = gbmGrid,
                 metric = "Kappa")
gbmFit3
```

```{r evaluate gbm3 model fit}
gbm3Classes <- predict(gbmFit3,newdata = testing)
str(gbm3Classes)
plot(gbmFit3)
gbm3Probs <- predict(gbmFit3,newdata = testing, type="prob")
head(gbm3Probs)
confusionMatrix(data=gbm3Classes, testing$Class)
```

The gradient boosted models lost performance with more interaction depth and tree size. The most simple model produced results closest to the support vector machine output. Both models misclassify 7 items out of the total test set of 62 items. The svm model is "better" at finding rocks while the gbm model predicts mine/metal more accurately.
